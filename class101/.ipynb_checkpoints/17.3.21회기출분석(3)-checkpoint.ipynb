{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21회 기출분석 (3) \n",
    "<br>\n",
    "## 2. 통계분석 (50점) <br>\n",
    "2. 연속형 독립변수 여러개의 소규모 데이터. (총 29점 )변수명은 순서대로 x1~x10 , 의미는 없는 데이터 <br>\n",
    "2-1. 데이터 8:2로 분할하고 선형회귀 적용하시오. 결정계수와 rmse 구하시오<br> <br>\n",
    "2-2. 데이터 8:2로 분할하고 릿지 회귀 적용하시오. <br>\n",
    "alpha 값을 0부터 1까지 0.1단위로 모두 탐색해서 결정계수가 가장 높을때의 알파를 찾고, 해당 알파로 다시 모델을 학습해서 결정계수와 rmse를 계산<br> <br>\n",
    "2-3. 데이터 8:2로 분할하고 라쏘 회귀 적용하시오.<br>\n",
    "alpha 값을 0부터 1까지 0.1단위로 모두 탐색해서 결정계수가 가장 높을때의 알파를 찾고, 해당 알파로 다시 모델을 학습해서 결정계수와 rmse를 계산<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import mglearn\n",
    "X,y = mglearn.datasets.load_extended_boston()\n",
    "# 훈련, 테스트 셋 분리 \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 데이터 8:2로 분할하고 선형회귀 적용하시오. 결정계수와 rmse 구하시오<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선형회귀 결정계수 :  0.6158858584078475\n",
      "선형회귀 RMSE :  5.592657237078841\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression() \n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "## R2 \n",
    "print(\"선형회귀 결정계수 : \", lr.score(X_test,y_test))\n",
    "print(\"선형회귀 RMSE : \", np.sqrt(mean_squared_error(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 데이터 8:2로 분할하고 릿지 회귀 적용하시오. <br>\n",
    "alpha 값을 0부터 1까지 0.1단위로 모두 탐색해서 결정계수가 가장 높을때의 알파를 찾고, 해당 알파로 다시 모델을 학습해서 결정계수와 rmse를 계산<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = np.arange(0,1.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Ridge(normalize=True),\n",
       "             param_grid={'alpha': array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(normalize=True) \n",
    "param_grid = {'alpha':alpha}\n",
    "ridge_model = GridSearchCV(ridge, param_grid)\n",
    "ridge_model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "릿지회귀 결정계수 :  0.746382410891928\n",
      "릿지회귀 RMSE :  4.544412437236793\n"
     ]
    }
   ],
   "source": [
    "print(\"릿지회귀 결정계수 : \", ridge_model.score(X_test,y_test))\n",
    "print(\"릿지회귀 RMSE : \", np.sqrt(mean_squared_error(y_test, ridge_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. 데이터 8:2로 분할하고 라쏘 회귀 적용하시오.<br>\n",
    "alpha 값을 0부터 1까지 0.1단위로 모두 탐색해서 결정계수가 가장 높을때의 알파를 찾고, 해당 알파로 다시 모델을 학습해서 결정계수와 rmse를 계산<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 749.6117813682623, tolerance: 2.837745529411765\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 713.0310617222682, tolerance: 2.8732755665634673\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 775.6180193404141, tolerance: 2.7303048482972136\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 643.4839241530722, tolerance: 2.593483919504644\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 858.1943953610436, tolerance: 2.7142222222222228\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\model_selection\\_search.py:880: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jjang\\anaconda3\\envs\\ADP_Class\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1040.4848910693343, tolerance: 3.4398062970297034\n",
      "  positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.0, normalize=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(normalize=True) \n",
    "param_grid = {'alpha':alpha}\n",
    "lasso_model = GridSearchCV(lasso, param_grid)\n",
    "lasso_model.fit(X_train,y_train)\n",
    "lasso_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라쏘회귀 결정계수 :  0.6901880385280015\n",
      "라쏘회귀 RMSE :  5.022698918447145\n"
     ]
    }
   ],
   "source": [
    "print(\"라쏘회귀 결정계수 : \", lasso_model.best_estimator_.score(X_test,y_test))\n",
    "print(\"라쏘회귀 RMSE : \", np.sqrt(mean_squared_error(y_test, lasso_model.best_estimator_.predict(X_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": "16",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "779px",
    "left": "120px",
    "top": "80px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
